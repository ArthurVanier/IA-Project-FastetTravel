{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded, 50 objects detected\n",
      "<spacy.pipeline.ner.EntityRecognizer object at 0x12f595070>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m     nlp\u001b[38;5;241m.\u001b[39mto_disk(output_dir)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained and saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(nbr_iteration)\u001b[0m\n\u001b[1;32m     33\u001b[0m training_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 35\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(entities)\n\u001b[1;32m     37\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mmake_doc(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "\n",
    "def train_model(nbr_iteration):\n",
    "\n",
    "    data = None\n",
    "    with open(\"training/spaCyFromToTraining.json\", \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        print(f\"Data Loaded, {len(data)} objects detected\")\n",
    "\n",
    "    \n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    # Check if NER component exists, if not, add it\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Add your custom labels (e.g., \"FROM\" and \"TO\") to the NER component\n",
    "    ner.add_label(\"FROM\")\n",
    "    ner.add_label(\"TO\")\n",
    "\n",
    "    print(ner)\n",
    "\n",
    "    # Prepare training examples\n",
    "    training_examples = []\n",
    "    for example in data:\n",
    "        entities = example.get(\"entities\", [])\n",
    "        print(entities)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        # Create a spaCy Example\n",
    "        example_dict = {\"text\": example[\"text\"], \"entities\": entities}\n",
    "        print(example_dict)\n",
    "        example_spacy = Example.from_dict(doc, example_dict)\n",
    "\n",
    "        training_examples.append(example_spacy)\n",
    "\n",
    "    # Train the model\n",
    "    for _ in range(nbr_iteration):\n",
    "        random.shuffle(training_examples)\n",
    "        for example in training_examples:\n",
    "            nlp.update([example], drop=0.5)\n",
    "\n",
    "    # Save the trained model\n",
    "    output_dir = \"./models/\"\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Model trained and saved to:\", output_dir)\n",
    "\n",
    "\n",
    "train_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.3888888888888889, Time: 0.8889641761779785 seconds\n",
      "Epoch: 1, Accuracy: 0.4148936170212766, Time: 0.8719291687011719 seconds\n",
      "Epoch: 2, Accuracy: 0.5824175824175825, Time: 0.8925278186798096 seconds\n",
      "Epoch: 3, Accuracy: 0.643979057591623, Time: 0.9625580310821533 seconds\n",
      "Epoch: 4, Accuracy: 0.6666666666666666, Time: 0.881091833114624 seconds\n",
      "Epoch: 5, Accuracy: 0.7171717171717171, Time: 0.885673999786377 seconds\n",
      "Epoch: 6, Accuracy: 0.7487437185929648, Time: 0.8825361728668213 seconds\n",
      "Epoch: 7, Accuracy: 0.7688442211055276, Time: 0.8674800395965576 seconds\n",
      "Epoch: 8, Accuracy: 0.7766497461928934, Time: 0.910210132598877 seconds\n",
      "Epoch: 9, Accuracy: 0.7076923076923077, Time: 0.8641369342803955 seconds\n",
      "Epoch: 10, Accuracy: 0.7738693467336684, Time: 0.9320383071899414 seconds\n",
      "Epoch: 11, Accuracy: 0.7788944723618091, Time: 0.8986010551452637 seconds\n",
      "Epoch: 12, Accuracy: 0.795, Time: 0.8798091411590576 seconds\n",
      "Epoch: 13, Accuracy: 0.8080808080808081, Time: 0.8794200420379639 seconds\n",
      "Epoch: 14, Accuracy: 0.8383838383838383, Time: 0.8588602542877197 seconds\n",
      "Epoch: 15, Accuracy: 0.803030303030303, Time: 0.8597168922424316 seconds\n",
      "Epoch: 16, Accuracy: 0.835, Time: 0.8672158718109131 seconds\n",
      "Epoch: 17, Accuracy: 0.8080808080808081, Time: 0.8578617572784424 seconds\n",
      "Epoch: 18, Accuracy: 0.8040201005025126, Time: 0.8594670295715332 seconds\n",
      "Epoch: 19, Accuracy: 0.83, Time: 0.8592078685760498 seconds\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import time\n",
    "from spacy.training.example import Example\n",
    "import json\n",
    "\n",
    "# Load training data\n",
    "with open(\"training/spaCyFromToTraining.json\", \"r\") as json_file:\n",
    "   training_data = json.load(json_file)\n",
    "\n",
    "# Load validation data\n",
    "with open(\"validation/spaCyFromToValidation.json\", \"r\") as json_file:\n",
    "   validation_data = json.load(json_file)\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize the NER component\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "for _, annotations in training_data:\n",
    "   for ent in annotations.get(\"entities\"):\n",
    "       ner.add_label(ent[2])\n",
    "\n",
    "# Function to check if two entities match\n",
    "def entity_match(pred_text, pred_start, pred_end, pred_label, true_start, true_end, true_label):\n",
    "   return abs(pred_start - true_start) <= 2 and \\\n",
    "          abs(pred_end - true_end) <= 2 and \\\n",
    "          pred_label == true_label\n",
    "\n",
    "# Train the model by epoch\n",
    "for i in range(20): # Change the number of epochs according to your needs\n",
    "   start_time = time.time()\n",
    "   for text, annotations in training_data:\n",
    "       # Create a spaCy Example\n",
    "       example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "       # Update the model with the training example\n",
    "       nlp.update([example], drop=0.5)\n",
    "   elapsed_time = time.time() - start_time\n",
    "\n",
    "   # Evaluate the model on the validation set\n",
    "   correct_predictions = 0\n",
    "   total_predictions = 0\n",
    "   for text, annotations in validation_data:\n",
    "       doc = nlp(text)\n",
    "       predicted_entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "       true_entities = annotations[\"entities\"]\n",
    "       for pred_text, pred_start, pred_end, pred_label in predicted_entities:\n",
    "           total_predictions += 1\n",
    "           if any(entity_match(pred_text, pred_start, pred_end, pred_label, true[0], true[1], true[2]) for true in true_entities):\n",
    "               correct_predictions += 1\n",
    "   accuracy = correct_predictions / total_predictions\n",
    "\n",
    "   print(f\"Epoch: {i}, Accuracy: {accuracy}, Time: {elapsed_time} seconds\")\n",
    "\n",
    "# Save the updated model\n",
    "nlp.to_disk(\"models/test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "n_iter = 10\n",
    "for _ in range(n_iter):\n",
    "    for example in training_data:\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "        example_spacy = Example.from_dict(doc, example)\n",
    "\n",
    "        # Update the model with the training example\n",
    "        nlp.update([example_spacy], drop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities:\n",
      "('amiens', 21, 27, 'FROM')\n",
      "('paris', 53, 58, 'FROM')\n",
      "('marseille', 72, 81, 'TO')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the trained spaCy model\n",
    "nlp = spacy.load(\"models/test\")\n",
    "\n",
    "# Example text for prediction\n",
    "text_to_predict = \"J'aime les pommes de amiens et je voudrais partir de paris pour aller Ã  marseille\"\n",
    "\n",
    "# Process the text using the loaded model\n",
    "doc = nlp(text_to_predict.lower())\n",
    "\n",
    "# Extract named entities from the processed document\n",
    "entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the extracted entities\n",
    "print(\"Extracted Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "fake = Faker(\"fr_FR\")  # French locale\n",
    "df = pd.read_csv(\"./format_src/format_data_station_city.csv\", sep=\";\")\n",
    "cities = df[\"City\"].tolist()\n",
    "def generate_training_data(nbr_data: int):\n",
    "    training_data = []\n",
    "\n",
    "    for _ in range(nbr_data):\n",
    "        from_city = random.choice(cities)\n",
    "        to_city = random.choice(cities)\n",
    "\n",
    "        # Ensure unique cities\n",
    "        while to_city == from_city:\n",
    "            to_city = random.choice(cities)\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            from_city, to_city = to_city, from_city\n",
    "        \n",
    "        is_order_1 = random.choice([True, False, True])\n",
    "\n",
    "        sentence_template_order_1 = random.choice([\n",
    "            \"Je veux voyager de {} Ã  {}.\",\n",
    "            \"il faut que j'aille Ã  {}, je suis Ã  {}\",\n",
    "            \"je dois faire le trajet {} {}\",\n",
    "            \"{} {}\",\n",
    "            \"je suis Ã  {} et j'aimerai allez Ã  {}\",\n",
    "            \"de {} comment se rendre Ã  {}\",\n",
    "            \"je suis perdu, je pense etre pas loin de {} et je dois allez Ã  {}\",\n",
    "            \"depuis hier je pense allez Ã  {}, mais j'ai peur du trajet depuis {}\",\n",
    "            \"gare de {} gare de {}\"\n",
    "            \n",
    "\n",
    "        ])\n",
    "\n",
    "        sentence_template_order_2 = random.choice([\n",
    "            \"j'aimme les Pommes de Amiens, mais je vais Ã  {} de {}\",\n",
    "            \"j'adore {}, comment y allez de {}\",\n",
    "            \"je dois me de rendre Ã  {} mais je suis Ã  {}\",\n",
    "            \"c'est super {}, comment y allez de {}\",\n",
    "            \"je suis pas loin de Paris mais je dois allez Ã  {} depuis {}\"\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        sentence = sentence_template_order_1.format(from_city, to_city) if is_order_1 else sentence_template_order_2.format(to_city, from_city)\n",
    "        \n",
    "        \n",
    "        start_from = sentence.find(from_city)\n",
    "        start_to = sentence.find(to_city)\n",
    "\n",
    "        entities = [\n",
    "                (start_from, start_from + len(from_city), \"FROM\"),\n",
    "                (start_to, start_to + len(to_city), \"TO\")\n",
    "        ]\n",
    "        training_data.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data = generate_training_data(100)\n",
    "\n",
    "with open(\"training/spaCyFromToTraining.json\", \"w+\") as file:\n",
    "    json.dump(training_data, file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m\n\u001b[1;32m     58\u001b[0m         validation_data\u001b[38;5;241m.\u001b[39mappend((sentence, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m: entities}))\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validation_data\n\u001b[0;32m---> 63\u001b[0m validation_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_validation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation/spaCyFromToValidation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     66\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(validation_data, file)\n",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m, in \u001b[0;36mgenerate_validation_data\u001b[0;34m(nbr_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_validation_data\u001b[39m(nbr_data: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./format_src/format_data_station_city.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     cities \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nbr_data):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_validation_data(nbr_data: int):\n",
    "    validation_data = []\n",
    "\n",
    "    df = pd.read_csv(\"./format_src/format_data_station_city.csv\", sep=\";\")\n",
    "    cities = df[\"City\"].tolist()\n",
    "    for _ in range(nbr_data):\n",
    "        from_city = random.choice(cities)\n",
    "        to_city = random.choice(cities)\n",
    "\n",
    "        # Ensure unique cities\n",
    "        while to_city == from_city:\n",
    "            to_city = random.choice(cities)\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            from_city, to_city = to_city, from_city\n",
    "        \n",
    "        is_order_1 = random.choice([True, False, True])\n",
    "\n",
    "        sentence_template_order_1 = random.choice([\n",
    "            \"Je veux voyager de {} Ã  {}.\",\n",
    "            \"il faut que j'aille Ã  {}, je suis Ã  {}\",\n",
    "            \"je dois faire le trajet {} {}\",\n",
    "            \"{} {}\",\n",
    "            \"je suis Ã  {} et j'aimerai allez Ã  {}\",\n",
    "            \"de {} comment se rendre Ã  {}\",\n",
    "            \"je suis perdu, je pense etre pas loin de {} et je dois allez Ã  {}\",\n",
    "            \"depuis hier je pense allez Ã  {}, mais j'ai peur du trajet depuis {}\",\n",
    "            \"gare de {} gare de {}\"\n",
    "            \n",
    "        ])\n",
    "\n",
    "        sentence_template_order_2 = random.choice([\n",
    "            \"j'aimme les Pommes de Amiens, mais je vais Ã  {} de {}\",\n",
    "            \"j'adore {}, comment y allez de {}\",\n",
    "            \"je dois me de rendre Ã  {} mais je suis Ã  {}\",\n",
    "            \"c'est super {}, comment y allez de {}\",\n",
    "            \"je suis pas loin de Paris mais je dois allez Ã  {} depuis {}\"\n",
    "\n",
    "        ])\n",
    "\n",
    "        sentence = sentence_template_order_1.format(from_city, to_city) if is_order_1 else sentence_template_order_2.format(to_city, from_city)\n",
    "        \n",
    "        start_from = sentence.find(from_city)\n",
    "        start_to = sentence.find(to_city)\n",
    "\n",
    "        # Check if the cities are found in the sentence\n",
    "        if start_from == -1 or start_to == -1:\n",
    "            continue\n",
    "\n",
    "        # Check if the indices match the actual positions of the cities in the sentence\n",
    "        if sentence[start_from:start_from+len(from_city)] != from_city or sentence[start_to:start_to+len(to_city)] != to_city:\n",
    "            continue\n",
    "\n",
    "        entities = [\n",
    "                (start_from, start_from + len(from_city), \"FROM\"),\n",
    "                (start_to, start_to + len(to_city), \"TO\")\n",
    "        ]\n",
    "        validation_data.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "    return validation_data\n",
    "\n",
    "\n",
    "validation_data = generate_validation_data(100)\n",
    "\n",
    "with open(\"validation/spaCyFromToValidation.json\", \"w+\") as file:\n",
    "    json.dump(validation_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_27144\\152893444.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  station = row[1].lower()\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_27144\\152893444.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  city = row[7].lower()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "\n",
    "def extract_unique_station_and_cities():\n",
    "    df = pd.read_csv('./src/liste-des-gares.csv', sep=';')\n",
    "    \n",
    "\n",
    "   \n",
    "    station_list = numpy.array([])\n",
    "    city_list = numpy.array([])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # Traitement des donnÃ©es\n",
    "        station = row[1].lower()\n",
    "        city = row[7].lower()\n",
    "\n",
    "        station_list = numpy.append(station_list, station)\n",
    "        city_list = numpy.append(city_list, city)\n",
    "    \n",
    "\n",
    "    df_res = pd.DataFrame({\n",
    "        \"Station\": station_list,\n",
    "        \"City\": city_list\n",
    "    })\n",
    "\n",
    "    df_res.to_csv('./format_src/format_data_station_city.csv', sep=\";\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "extract_unique_station_and_cities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
