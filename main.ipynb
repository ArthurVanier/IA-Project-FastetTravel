{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parlez maintenant...\n",
      "Vous avez dit : bonjour J'aimerais procéder à un trajet de Paris pour aller à Amiens ça regarde-moi ça\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialiser le recogniteur\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Utiliser le microphone comme source audio\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Parlez maintenant...\")\n",
    "    audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        # Reconnaissance vocale en utilisant Google Web Speech API\n",
    "        text = recognizer.recognize_google(audio, language=\"fr-FR\")\n",
    "        print(\"Vous avez dit : \" + text)\n",
    "        # Export the text in /tmp/speech_recognition.txt\n",
    "        with open(\"/tmp/speech_recognition.txt\", \"w\") as f:\n",
    "            f.write(text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Web Speech API n'a pas pu comprendre l'audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"La requête vers Google Web Speech API a échoué; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.4046242774566474, Time: 0.9231841564178467 seconds\n",
      "Epoch: 1, Accuracy: 0.3523809523809524, Time: 0.9112780094146729 seconds\n",
      "Epoch: 2, Accuracy: 0.5306122448979592, Time: 0.9263458251953125 seconds\n",
      "Epoch: 3, Accuracy: 0.6113989637305699, Time: 1.1015348434448242 seconds\n",
      "Epoch: 4, Accuracy: 0.7329842931937173, Time: 0.9716470241546631 seconds\n",
      "Epoch: 5, Accuracy: 0.7589743589743589, Time: 0.8939340114593506 seconds\n",
      "Epoch: 6, Accuracy: 0.797979797979798, Time: 0.9421510696411133 seconds\n",
      "Epoch: 7, Accuracy: 0.8080808080808081, Time: 0.9364850521087646 seconds\n",
      "Epoch: 8, Accuracy: 0.8061224489795918, Time: 0.938319206237793 seconds\n",
      "Epoch: 9, Accuracy: 0.8121827411167513, Time: 0.8902640342712402 seconds\n",
      "Epoch: 10, Accuracy: 0.8186528497409327, Time: 0.9367609024047852 seconds\n",
      "Epoch: 11, Accuracy: 0.8223350253807107, Time: 0.9521999359130859 seconds\n",
      "Epoch: 12, Accuracy: 0.8527918781725888, Time: 0.9784502983093262 seconds\n",
      "Epoch: 13, Accuracy: 0.855, Time: 0.9187877178192139 seconds\n",
      "Epoch: 14, Accuracy: 0.7989949748743719, Time: 0.9270720481872559 seconds\n",
      "Epoch: 15, Accuracy: 0.826530612244898, Time: 0.9359197616577148 seconds\n",
      "Epoch: 16, Accuracy: 0.8442211055276382, Time: 0.8900599479675293 seconds\n",
      "Epoch: 17, Accuracy: 0.82, Time: 0.9911210536956787 seconds\n",
      "Epoch: 18, Accuracy: 0.82, Time: 0.9761919975280762 seconds\n",
      "Epoch: 19, Accuracy: 0.8258706467661692, Time: 0.9681949615478516 seconds\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import time\n",
    "from spacy.training.example import Example\n",
    "import json\n",
    "\n",
    "# Load training data\n",
    "with open(\"training/spaCyFromToTraining.json\", \"r\") as json_file:\n",
    "   training_data = json.load(json_file)\n",
    "\n",
    "# Load validation data\n",
    "with open(\"validation/spaCyFromToValidation.json\", \"r\") as json_file:\n",
    "   validation_data = json.load(json_file)\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize the NER component\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "for _, annotations in training_data:\n",
    "   for ent in annotations.get(\"entities\"):\n",
    "       ner.add_label(ent[2])\n",
    "\n",
    "# Function to check if two entities match\n",
    "def entity_match(pred_text, pred_start, pred_end, pred_label, true_start, true_end, true_label):\n",
    "   return abs(pred_start - true_start) <= 2 and \\\n",
    "          abs(pred_end - true_end) <= 2 and \\\n",
    "          pred_label == true_label\n",
    "\n",
    "# Train the model by epoch\n",
    "for i in range(20): # Change the number of epochs according to your needs\n",
    "   start_time = time.time()\n",
    "   for text, annotations in training_data:\n",
    "       # Create a spaCy Example\n",
    "       example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "       # Update the model with the training example\n",
    "       nlp.update([example], drop=0.5)\n",
    "   elapsed_time = time.time() - start_time\n",
    "\n",
    "   # Evaluate the model on the validation set\n",
    "   correct_predictions = 0\n",
    "   total_predictions = 0\n",
    "   for text, annotations in validation_data:\n",
    "       doc = nlp(text)\n",
    "       predicted_entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "       true_entities = annotations[\"entities\"]\n",
    "       for pred_text, pred_start, pred_end, pred_label in predicted_entities:\n",
    "           total_predictions += 1\n",
    "           if any(entity_match(pred_text, pred_start, pred_end, pred_label, true[0], true[1], true[2]) for true in true_entities):\n",
    "               correct_predictions += 1\n",
    "   accuracy = correct_predictions / total_predictions\n",
    "\n",
    "   print(f\"Epoch: {i}, Accuracy: {accuracy}, Time: {elapsed_time} seconds\")\n",
    "\n",
    "# Save the updated model\n",
    "nlp.to_disk(\"models/test/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities:\n",
      "('toulouse', 94, 102, 'TO')\n",
      "('caen', 118, 122, 'TO')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the trained spaCy model\n",
    "nlp = spacy.load(\"models/test\")\n",
    "\n",
    "# Example text for prediction\n",
    "# Load the text from /tmp/speech_recognition.txt\n",
    "with open(\"/tmp/speech_recognition.txt\", \"r\") as f:\n",
    "    text_to_predict = f.read()\n",
    "\n",
    "# Process the text using the loaded model\n",
    "doc = nlp(text_to_predict.lower())\n",
    "\n",
    "# Extract named entities from the processed document\n",
    "entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the extracted entities\n",
    "print(\"Extracted Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "fake = Faker(\"fr_FR\")  # French locale\n",
    "df = pd.read_csv(\"./format_src/format_data_station_city.csv\", sep=\";\")\n",
    "cities = df[\"City\"].tolist()\n",
    "def generate_training_data(nbr_data: int):\n",
    "    training_data = []\n",
    "\n",
    "    for _ in range(nbr_data):\n",
    "        from_city = random.choice(cities)\n",
    "        to_city = random.choice(cities)\n",
    "\n",
    "        # Ensure unique cities\n",
    "        while to_city == from_city:\n",
    "            to_city = random.choice(cities)\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            from_city, to_city = to_city, from_city\n",
    "        \n",
    "        is_order_1 = random.choice([True, False, True])\n",
    "\n",
    "        sentence_template_order_1 = random.choice([\n",
    "            \"Je veux voyager de {} à {}.\",\n",
    "            \"il faut que j'aille à {}, je suis à {}\",\n",
    "            \"je dois faire le trajet {} {}\",\n",
    "            \"{} {}\",\n",
    "            \"je suis à {} et j'aimerai allez à {}\",\n",
    "            \"de {} comment se rendre à {}\",\n",
    "            \"je suis perdu, je pense etre pas loin de {} et je dois allez à {}\",\n",
    "            \"depuis hier je pense allez à {}, mais j'ai peur du trajet depuis {}\",\n",
    "            \"gare de {} gare de {}\"\n",
    "            \n",
    "\n",
    "        ])\n",
    "\n",
    "        sentence_template_order_2 = random.choice([\n",
    "            \"j'aimme les Pommes de Amiens, mais je vais à {} de {}\",\n",
    "            \"j'adore {}, comment y allez de {}\",\n",
    "            \"je dois me de rendre à {} mais je suis à {}\",\n",
    "            \"c'est super {}, comment y allez de {}\",\n",
    "            \"je suis pas loin de Paris mais je dois allez à {} depuis {}\"\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        sentence = sentence_template_order_1.format(from_city, to_city) if is_order_1 else sentence_template_order_2.format(to_city, from_city)\n",
    "        \n",
    "        \n",
    "        start_from = sentence.find(from_city)\n",
    "        start_to = sentence.find(to_city)\n",
    "\n",
    "        entities = [\n",
    "                (start_from, start_from + len(from_city), \"FROM\"),\n",
    "                (start_to, start_to + len(to_city), \"TO\")\n",
    "        ]\n",
    "        training_data.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data = generate_training_data(100)\n",
    "\n",
    "with open(\"training/spaCyFromToTraining.json\", \"w+\") as file:\n",
    "    json.dump(training_data, file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_data(nbr_data: int):\n",
    "    validation_data = []\n",
    "\n",
    "    df = pd.read_csv(\"./format_src/format_data_station_city.csv\", sep=\";\")\n",
    "    cities = df[\"City\"].tolist()\n",
    "    for _ in range(nbr_data):\n",
    "        from_city = random.choice(cities)\n",
    "        to_city = random.choice(cities)\n",
    "\n",
    "        # Ensure unique cities\n",
    "        while to_city == from_city:\n",
    "            to_city = random.choice(cities)\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            from_city, to_city = to_city, from_city\n",
    "        \n",
    "        is_order_1 = random.choice([True, False, True])\n",
    "\n",
    "        sentence_template_order_1 = random.choice([\n",
    "            \"Je veux voyager de {} à {}.\",\n",
    "            \"il faut que j'aille à {}, je suis à {}\",\n",
    "            \"je dois faire le trajet {} {}\",\n",
    "            \"{} {}\",\n",
    "            \"je suis à {} et j'aimerai allez à {}\",\n",
    "            \"de {} comment se rendre à {}\",\n",
    "            \"je suis perdu, je pense etre pas loin de {} et je dois allez à {}\",\n",
    "            \"depuis hier je pense allez à {}, mais j'ai peur du trajet depuis {}\",\n",
    "            \"gare de {} gare de {}\"\n",
    "            \n",
    "        ])\n",
    "\n",
    "        sentence_template_order_2 = random.choice([\n",
    "            \"j'aimme les Pommes de Amiens, mais je vais à {} de {}\",\n",
    "            \"j'adore {}, comment y allez de {}\",\n",
    "            \"je dois me de rendre à {} mais je suis à {}\",\n",
    "            \"c'est super {}, comment y allez de {}\",\n",
    "            \"je suis pas loin de Paris mais je dois allez à {} depuis {}\"\n",
    "\n",
    "        ])\n",
    "\n",
    "        sentence = sentence_template_order_1.format(from_city, to_city) if is_order_1 else sentence_template_order_2.format(to_city, from_city)\n",
    "        \n",
    "        start_from = sentence.find(from_city)\n",
    "        start_to = sentence.find(to_city)\n",
    "\n",
    "        # Check if the cities are found in the sentence\n",
    "        if start_from == -1 or start_to == -1:\n",
    "            continue\n",
    "\n",
    "        # Check if the indices match the actual positions of the cities in the sentence\n",
    "        if sentence[start_from:start_from+len(from_city)] != from_city or sentence[start_to:start_to+len(to_city)] != to_city:\n",
    "            continue\n",
    "\n",
    "        entities = [\n",
    "                (start_from, start_from + len(from_city), \"FROM\"),\n",
    "                (start_to, start_to + len(to_city), \"TO\")\n",
    "        ]\n",
    "        validation_data.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "    return validation_data\n",
    "\n",
    "\n",
    "validation_data = generate_validation_data(100)\n",
    "\n",
    "with open(\"validation/spaCyFromToValidation.json\", \"w+\") as file:\n",
    "    json.dump(validation_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/51xkbm4139b206gcdc8fxpww0000gn/T/ipykernel_1606/152893444.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  station = row[1].lower()\n",
      "/var/folders/_z/51xkbm4139b206gcdc8fxpww0000gn/T/ipykernel_1606/152893444.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  city = row[7].lower()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "\n",
    "def extract_unique_station_and_cities():\n",
    "    df = pd.read_csv('./src/liste-des-gares.csv', sep=';')\n",
    "    \n",
    "\n",
    "   \n",
    "    station_list = numpy.array([])\n",
    "    city_list = numpy.array([])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # Traitement des données\n",
    "        station = row[1].lower()\n",
    "        city = row[7].lower()\n",
    "\n",
    "        station_list = numpy.append(station_list, station)\n",
    "        city_list = numpy.append(city_list, city)\n",
    "    \n",
    "\n",
    "    df_res = pd.DataFrame({\n",
    "        \"Station\": station_list,\n",
    "        \"City\": city_list\n",
    "    })\n",
    "\n",
    "    df_res.to_csv('./format_src/format_data_station_city.csv', sep=\";\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "extract_unique_station_and_cities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
