{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded, 50 objects detected\n",
      "<spacy.pipeline.ner.EntityRecognizer object at 0x0000018D56BA3990>\n",
      "[{'start': 8, 'end': 16, 'label': 'FROM'}, {'start': 25, 'end': 44, 'label': 'TO'}]\n",
      "{'text': 'gare de Kleindan gare de Delorme-sur-Lacroix', 'entities': [{'start': 8, 'end': 16, 'label': 'FROM'}, {'start': 25, 'end': 44, 'label': 'TO'}]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E973] Unexpected type for NER data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m     nlp\u001b[38;5;241m.\u001b[39mto_disk(output_dir)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained and saved to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_dir)\n\u001b[1;32m---> 58\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(nbr_iteration)\u001b[0m\n\u001b[0;32m     40\u001b[0m     example_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m: entities}\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(example_dict)\n\u001b[1;32m---> 42\u001b[0m     example_spacy \u001b[38;5;241m=\u001b[39m \u001b[43mExample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     training_examples\u001b[38;5;241m.\u001b[39mappend(example_spacy)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\spacy\\training\\example.pyx:128\u001b[0m, in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\spacy\\training\\example.pyx:34\u001b[0m, in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\spacy\\training\\example.pyx:526\u001b[0m, in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E973] Unexpected type for NER data"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "\n",
    "def train_model(nbr_iteration):\n",
    "\n",
    "    data = None\n",
    "    with open(\"training/spaCyFromToTraining.json\", \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        print(f\"Data Loaded, {len(data)} objects detected\")\n",
    "\n",
    "    \n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "    # Check if NER component exists, if not, add it\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Add your custom labels (e.g., \"FROM\" and \"TO\") to the NER component\n",
    "    ner.add_label(\"FROM\")\n",
    "    ner.add_label(\"TO\")\n",
    "\n",
    "    print(ner)\n",
    "\n",
    "    # Prepare training examples\n",
    "    training_examples = []\n",
    "    for example in data:\n",
    "        entities = example.get(\"entities\", [])\n",
    "        print(entities)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        # Create a spaCy Example\n",
    "        example_dict = {\"text\": example[\"text\"], \"entities\": entities}\n",
    "        print(example_dict)\n",
    "        example_spacy = Example.from_dict(doc, example_dict)\n",
    "\n",
    "        training_examples.append(example_spacy)\n",
    "\n",
    "    # Train the model\n",
    "    for _ in range(nbr_iteration):\n",
    "        random.shuffle(training_examples)\n",
    "        for example in training_examples:\n",
    "            nlp.update([example], drop=0.5)\n",
    "\n",
    "    # Save the trained model\n",
    "    output_dir = \"./models/\"\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Model trained and saved to:\", output_dir)\n",
    "\n",
    "\n",
    "train_model(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded, 50 objects detected\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "data = None\n",
    "with open(\"training/spaCyFromToTraining.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    print(f\"Data Loaded, {len(data)} objects detected\")\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Example training data\n",
    "training_data = [\n",
    "    (\n",
    "        \"Apple est une entreprise américaine.\",\n",
    "        {\"entities\": [(0, 5, \"ORG\")]}\n",
    "    ),\n",
    "    # Add more examples as needed\n",
    "]\n",
    "\n",
    "# Process and update the model with training data\n",
    "for _ in range(100):\n",
    "    for text, annotations in data:\n",
    "        # Create a spaCy Example\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        # Update the model with the training example\n",
    "        nlp.update([example], drop=0.5)\n",
    "\n",
    "# Save the updated model\n",
    "nlp.to_disk(\"models/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "n_iter = 10\n",
    "for _ in range(n_iter):\n",
    "    for example in training_data:\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "        example_spacy = Example.from_dict(doc, example)\n",
    "\n",
    "        # Update the model with the training example\n",
    "        nlp.update([example_spacy], drop=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Entities:\n",
      "('Marseille', 10, 19, 'TO')\n",
      "('Nice', 27, 31, 'TO')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the trained spaCy model\n",
    "nlp = spacy.load(\"models/test\")\n",
    "\n",
    "# Example text for prediction\n",
    "text_to_predict = \"Je vais à Marseille depuis Nice.\"\n",
    "\n",
    "# Process the text using the loaded model\n",
    "doc = nlp(text_to_predict)\n",
    "\n",
    "# Extract named entities from the processed document\n",
    "entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the extracted entities\n",
    "print(\"Extracted Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "\n",
    "fake = Faker(\"fr_FR\")  # French locale\n",
    "\n",
    "def generate_training_data(nbr_data: int):\n",
    "    training_data = []\n",
    "\n",
    "    for _ in range(nbr_data):\n",
    "        from_city = fake.city()\n",
    "        to_city = fake.city()\n",
    "\n",
    "        # Ensure unique cities\n",
    "        while to_city == from_city:\n",
    "            to_city = fake.city()\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            from_city, to_city = to_city, from_city\n",
    "        \n",
    "        is_order_1 = random.choice([True, False, True])\n",
    "\n",
    "        sentence_template_order_1 = random.choice([\n",
    "            \"Je veux voyager de {} à {}.\",\n",
    "            \"il faut que j'aille à {}, je suis à {}\",\n",
    "            \"je dois faire le trajet {} {}\",\n",
    "            \"{} {}\",\n",
    "            \"je suis à {} et j'aimerai allez à {}\",\n",
    "            \"de {} comment se rendre à {}\",\n",
    "            \"je suis perdu, je pense etre pas loin de {} et je dois allez à {}\",\n",
    "            \"depuis hier je pense allez à {}, mais j'ai peur du trajet depuis {}\",\n",
    "            \"gare de {} gare de {}\"\n",
    "            \n",
    "\n",
    "        ])\n",
    "\n",
    "        sentence_template_order_2 = random.choice([\n",
    "            \"j'aimme les Pommes de Amiens, mais je vais à {} de {}\",\n",
    "            \"j'adore {}, comment y allez de {}\",\n",
    "            \"je dois me de rendre à {} mais je suis à {}\",\n",
    "            \"c'est super {}, comment y allez de {}\",\n",
    "            \"je suis pas loin de Paris mais je dois allez à {} depuis {}\"\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "        sentence = sentence_template_order_1.format(from_city, to_city) if is_order_1 else sentence_template_order_2.format(to_city, from_city)\n",
    "        \n",
    "        \n",
    "        start_from = sentence.find(from_city)\n",
    "        start_to = sentence.find(to_city)\n",
    "\n",
    "        entities = [\n",
    "                (start_from, start_from + len(from_city), \"FROM\"),\n",
    "                (start_to, start_to + len(to_city), \"TO\")\n",
    "        ]\n",
    "        training_data.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data = generate_training_data(50)\n",
    "\n",
    "with open(\"training/spaCyFromToTraining.json\", \"w+\") as file:\n",
    "    json.dump(training_data, file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_27144\\152893444.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  station = row[1].lower()\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_27144\\152893444.py:18: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  city = row[7].lower()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "\n",
    "def extract_unique_station_and_cities():\n",
    "    df = pd.read_csv('./src/liste-des-gares.csv', sep=';')\n",
    "    \n",
    "\n",
    "   \n",
    "    station_list = numpy.array([])\n",
    "    city_list = numpy.array([])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        # Traitement des données\n",
    "        station = row[1].lower()\n",
    "        city = row[7].lower()\n",
    "\n",
    "        station_list = numpy.append(station_list, station)\n",
    "        city_list = numpy.append(city_list, city)\n",
    "    \n",
    "\n",
    "    df_res = pd.DataFrame({\n",
    "        \"Station\": station_list,\n",
    "        \"City\": city_list\n",
    "    })\n",
    "\n",
    "    df_res.to_csv('./format_src/format_data_station_city.csv', sep=\";\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "extract_unique_station_and_cities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
